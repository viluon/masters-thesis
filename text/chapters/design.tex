%---------------%
\chapter{Design}
%---------------%

The high-level architecture of the \acrshort{p4} Analyzer project marks a
departure from conventional language server designs in that it primarily targets
WebAssembly and aims to run entirely within the Visual Studio Code editor. This
decision makes installation simpler for the end-user, cross-platform support
easier for the developers, and security policy conformance trivial for any
security teams involved.

The main mode of operation is thus as follows: the language server runs in a
WebAssembly worker of the \acrshort{p4} Analyzer VS Code extension. The
extension itself defines a simple TextMate\cite{textmate} grammar specification
and serves as a thin client for the server. The main bulk of \acrshort{lsp}
functionality is delegated to the editor. VS Code forwards edits to open files
to the language server, which updates its model of the workspace. When VS Code
asks for completions, hover, diagnostics, or other features, the analyzer
recomputes necessary information on-demand and responds appropriately.

In addition to the WebAssembly executable, the \acrshort{p4} Analyzer project
also compiles to a native binary that executes in a standalone process and
communicates with an arbitrary \acrshort{lsp}-compliant client over a socket.
However, the standalone language server requires support for certain features
related to filesystem functionality that fall outside the protocol
specification. We will discuss these in-depth later\todo{make sure this is
indeed the case, put a reference here}.

%-----------------------------------------------%
\section{The \pdfacrshort{p4} Analyzer pipeline}
%-----------------------------------------------%

The first step in our pipeline is lexical analysis. Somewhat unconventionally,
our lexer produces tokens even for the preprocessor (i.e. it analyses
preprocessor directives). Our preprocessor then operates at the lexeme level,
rather than running separately as the first step. This requires a
reimplementation of the preprocessor, which is already necessitated by
fault-tolerance and WebAssembly support requirements anyway. On the upside, a
custom preprocessor simplifies tracking of source positions, which are crucial
for accurate diagnostics.

\subsection{Lexical analysis}

The \pfs specification defines a \acrshort{yacc}/Bison grammar for the language.
However, this grammar has several flaws.

For example, it reuses the \texttt{parserTypeDeclaration} non\-terminal in
\texttt{parserDeclaration}s but imposes extra restrictions: a parser declaration
may not be generic. This requires checking the child production outside the
grammar specification.

However, the primary issue is that the grammar design does not maintain a clean
separation between a parser and a lexer and requires these two components to
collaborate.

\begin{displayquote}
	\textit{The grammar is actually ambiguous, so the lexer and the parser must
	collaborate for parsing the language. In particular, the lexer must be able
	to distinguish two kinds of identifiers:}

	\begin{itemize}
		\item \textit{Type names previously introduced (\texttt{TYPE\_IDENTIFIER}
		tokens)}
		\item \textit{Regular identifiers (\texttt{IDENTIFIER} token)}
	\end{itemize}

	\textit{The parser has to use a symbol table to indicate to the lexer how to
	parse subsequent appearances of identifiers.}

	-- \citetitle{p416:v123:spec} \cite{p416:v123:spec}
\end{displayquote}

The specification goes on to show an example where the lexer output depends on
the parser state and mentions that the presented grammar ``\textit{has been heavily
influenced by limitations of the Bison parser generator tool.}''

The tight coupling between the lexer and the parser, as well as the decision to
remain in the confines of an outdated parser generator despite its many
drawbacks, are in our opinion examples of poor design for a language born in the
twenty-first century. We have elected not to follow this ambiguous grammar
specification in the \acrshort{p4} Analyzer project and instead build a pipeline
that is tolerant to invalid input to the fullest extent possible, while
accepting the same language.

Our lexer's task is to convert the input string into a stream of lexemes. The
lexer is a standalone finite state machine independent of any later stages in
the pipeline. It has a secondary output for reporting diagnostics, but this side
channel is write-only.

\subsubsection*{Error tolerance}

Error tolerance at the lexer level means proceeding with lexeme stream
generation despite nonsensical input. We emit a special error token whenever
such input is encountered. Additionally, the lexer validates numeric constants,
which can specify width, base, and signedness. These properties could be out of
bounds for a given literal. In these cases, the lexer should still produce a
valid token while logging an error-level diagnostic. The server can then report
the diagnostic to the user once lexing completes.


\subsection{The preprocessor}

\pfs requires support for a preprocessor, very similar to the C preprocessor,
directly in the specification. However, it does not ask implementors to support
the entirety of \texttt{cpp}. Notably, only simple, parameter-less macros are
allowed. This is already enough to necessitate running the preprocessor before
starting the parser, however. Consider the code in Listing~\ref{lst:p4pp}. These
examples show how grammatically invalid code may become valid and vice versa,
based only on the right-hand sides of preprocessor macros.

An important consideration for a correct implementation of preprocessor
directives is their context-sensitive nature. Expressions for conditional
inclusion in directives \texttt{\#if}, \texttt{\#elif}, and \texttt{\#ifdef} are
themselves subject to macro substitution and thus have to be kept in plain text
or lexeme form until their evaluation.

One more thing to note here is the mechanism of document inclusion. Before
analysing a \pfs source file (at least to some degree), the full extent of its
dependencies is unknown and arbitrary. The language has no module system and
imposes no restriction on the paths a source file can include. This poses a
challenge for lexeme-level preprocessors, as a file needs to be lexed before it
can be included. To deal with this, a correct implementation should collect the
paths a source file can depend on, lex their contents, and include their lexemes
in the preprocessed lexeme stream. This is of particular note in our
implementation, as the collection of dependencies reports this dependency set to
the editor to set up filesystem-level watches. Subsequent edits to the
dependencies, or even to the dependency set itself, can be processed
incrementally.

\subsubsection*{Error tolerance}

Error tolerance in the preprocessor means reporting errors and warnings about
malformed input to the user while continuing to interpret directives in the
input stream on a best-effort basis. Mistakes in preprocessor directives come in
several flavours.

The directive itself may be malformed, either due to a typo in its name or a
problem in some of its arguments. The former case will simply be lexed as an
unrecognized directive and reported as such. It is possible to suggest fixes for
common typos to the user. A problem in the directive's argument or arguments
needs to be resolved based on its meaning. For example, an \texttt{\#include}
directive could point to a non-existent file, the preprocessor should then
report this error and proceed as if the file were not included. This is likely
to lead to further errors down the road, but without knowledge of the referenced
file's contents, it is the best a preprocessor can do.

Another class of errors is semantic and context-sensitive in nature: a directive
may be used in the incorrect context or missing where it is expected. For
example, a user may forget to add an \texttt{\#endif} directive, or include more
than one \texttt{\#else} directive for a condition. Unfortunately, guessing the
user's intention when faced with any syntactic or semantic problems in the input
is a tall order. No guarantees of optimality can be given, as is often the case
with similar heuristics. In the duplicate \texttt{\#else} problem, the
preprocessor could be reasonably expected to either skip over the first
\texttt{\#else}'s body, the second \texttt{\#else}'s body, or assume either of
the directives was inserted by accident and pretend it is not a part of the
input stream. We choose to skip the second \texttt{\#else}'s body in our design,
but other strategies are equally valid.

\begin{lstlisting}[
	caption={~\pfs preprocessor example},
	label=lst:p4pp,
	captionpos=t,
	tabsize=4,
	float,
	abovecaptionskip=-\medskipamount,
	belowcaptionskip=\medskipamount,
	language=p4
]
#define op +
// #define op 2

#define paren )

header h {
	bit<1> field;
}

control pipe(inout h hdr) {
	Checksum16() ck;
	apply {
		// arithmetic expression could be invalid
		h.field = 1 op 3;
		// a parse without prior macro substitution would fail
		ck.clear( paren;
		// this would parse correctly, but macro substitution
		// will reveal a parse error
		ck.update(op);
	}
}
\end{lstlisting}


\subsection{The parser}

The next natural step in the pipeline is the act of finding the productions of a
\pfs grammar that match the preprocessed input program; parsing. While the steps
up to this point are fairly simplistic and efficient, parsing is a
resource-intensive process. A language server is expected to provide real-time
feedback to the developer, including auto-completion suggestions updated with
every keypress. Low latency is crucial to the end-user and the parser lies on
every critical path from user input to high-level results shown in the editor's
interface. At the same time, a typical \pfs program is likely to consist of a
long prefix that does not change between edits and a user-maintained suffix that
changes frequently. This is because a \pfs program usually begins with
\texttt{\#include} directives referencing platform-specific files with
constants, error codes, \extern{} definitions and other shared code. These
constraints and conditions are a very good fit for the field of
\emph{incremental parsing}.

An incremental parser aims to reuse previously computed information about the
input in response to small perturbations. Our parser specifically builds on
incremental packrat parsing\cite{dubroy2017incremental-packrat-parsing}, which
places few constraints on grammar design and is easy to implement in an
extensible manner.

Packrat parsing\cite{ford2002packrat} is a linear time algorithm for recognizing
productions of a \acrfull{peg}. It relies heavily on memoization to avoid costly
backtracking, at the expense of memory overhead.
\linkedciteauthor{dubroy2017incremental-packrat-parsing} augment the packrat
memoization table to support incrementality. The result is a parser that is at
once simple, general, incremental, and efficient.

Our packrat parser conceptually handles \acrlong{peg}s\cite{ford2004parsing}, a
class of unambiguous grammars for context-free languages. \acrshort{peg}s are
syntactically similar to \acrlong{cfg}s. However, the choice operator in
\acrshort{cfg}s is ambiguous, whereas \acrshort{peg}s use ordered choice, which
greedily attempts to match alternatives in order. The right-hand side of a
\acrlong{peg} rule can also contain predicates, which attempt to match without
consuming input. Predicates are useful for positive and negative look\-ahead.

\begin{figure}[h]
	\centering
	\begin{align*}
		e ::= & \hspace{2pt} \varepsilon & \text{(empty string)} \\
		\mid  & \hspace{2pt} \texttt{t}  & \text{(terminal)} \\
		\mid  & \hspace{2pt} e_1 e_2     & \text{(sequence)} \\
		\mid  & \hspace{2pt} e_1 | e_2   & \text{(ordered choice)} \\
		\mid  & \hspace{2pt} e^*         & \text{(zero or more)} \\
		\mid  & \hspace{2pt} e^+         & \text{(one or more)} \\
		\mid  & \hspace{2pt} e?          & \text{(zero or one)} \\
		\mid  & \hspace{2pt} \&e         & \text{(positive lookahead)} \\
		\mid  & \hspace{2pt} !e          & \text{(negative lookahead)} \\
		\texttt{t} \in & \hspace{2pt} \text{tokens}
	\end{align*}
	\caption{Syntax of \acrlong{peg}s.}
	\label{fig:peg-syntax}
\end{figure}

The syntax of typical \acrlong{peg}s can be seen in Figure~\ref{fig:peg-syntax}.
Our grammars are slightly simpler: we implement neither positive lookahead nor
the $+$ and $?$ operators. Positive lookahead can be simulated by nesting two
negative lookaheads, and the $+$ and $?$ operators can desugar to combinations
of general repetition, ordered choice, and the empty string. These decisions
were made to simplify the parser implementation, but in turn complicate the
grammar with verbose and repetitive definitions. It remains to be seen whether
they survive future refactorings. A further restriction is that all grammar
rules must contain at most one level of nesting, i.e. the grammar must be given
in a normal form where the immediate subtrees of a rule's right-hand side are
all non-terminals.

Our design differentiates between a generic parser library and a parser built on
it. Grammars are defined using a small \acrshort{dsl} implemented with Rust's
declarative macros. An example can be seen in
Listing~\ref{lst:grammar-dsl-example}. The~\texttt{grammar!} macro expands to a
data structure representing the grammar itself. This structure can be passed to
a smart constructor, which validates the grammar\footnote{Ensuring all
referenced non-terminals are in fact defined, and that \texttt{start} is
present.} and returns a parser. The implementation interprets the grammar
\acrshort{dsl} at runtime.

If future testing and development necessitate optimization of the parser, there
is room to build a parser compiler for the \acrshort{dsl} and generate a more
efficient solution from the same grammar. Procedural Rust macros\footnote{While
declarative macros can only perform a very restricted set of rewriting
operations on token trees, procedural macros can run arbitrary Rust code during
expansion.} could take care of integrating the parser compiler into the build
process.

\subsubsection*{Incremental updates}

Since the parser is required to process incremental updates to the input
sequence, it is not simply a function from a sequence of tokens to a parse tree.
Rather, the parser takes a reference to a read-write lock of the input. It
defines an \texttt{apply\_edit} method that acquires a write lock of the input
sequence, applies the change, invalidates relevant entries in the memoization
table, and releases the lock.

To initiate parsing, the user invokes the \texttt{parse} method, which acquires
a read lock on the input for the duration of parsing.


\begin{lstlisting}[
	caption={~Example grammar in our \acrshort{dsl}.},
	label=lst:grammar-dsl-example,
	captionpos=t,
	tabsize=4,
	float,
	abovecaptionskip=-\medskipamount,
	belowcaptionskip=\medskipamount,
	language=c
]
grammar! {
	// The initial non-terminal is called `start`
	start => p4program;
	// The postfix `rep` operator corresponds to Kleene star
	ws => whitespace rep;
	// Non-terminals can expand to terminals by wrapping the terminal in
	// parentheses
	whitespace => (Token::Whitespace);

	// The right hand side can also be a sequence separated by commas
	p4program => ws, top_level_decls, ws;
	// ...or a choice separated by pipes
	top_level_decls =>
		top_level_decls_rep | top_level_decls_end | nothing;
	top_level_decls_rep => top_level_decl, ws, top_level_decls;
	top_level_decls_end => (Token::Semicolon);

	direction => dir_in | dir_out | dir_inout;
	// Rules can also match tokens against an arbitrary Rust pattern,
	// which is useful for identifying soft keywords
	dir_in    => { Token::Identifier(i) if i == "in" };
	dir_out   => { Token::Identifier(i) if i == "out" };
	dir_inout => { Token::Identifier(i) if i == "inout" };
}
\end{lstlisting}


\subsubsection*{Error reporting}

Error handling in parsing is far more nuanced than in any of the previous steps,
which is not surprising, considering the relative complexity of the languages
that the individual steps recognize and process. A good parser should attempt to
provide as much feedback as possible to the user, even when faced with
unexpected tokens. It is not enough to simply stop at the first error, and it is
not enough to be imprecise about the locations of problems in the source file.
Both of these considerations pose some challenges.

The desire to continue parsing malformed input to provide feedback to the
developer has a long history\cite{graham1975practical}. Error recovery has been
studied at length in \acrlong{peg}s as well\cite{redziejowski2009mouse,
maidl2013exception, demedeiros2016parsing, demedeiros2018syntax,
demedeiros2020automatic}. The \acrshort{peg} case is interesting, because a
packrat parser relies on failures to guide choice selection. By the time a
parsing failure propagates to the starting non-terminal, the information about
the context that led to it is lost.

Specifically, when encountering unexpected input, a packrat parser unwinds the
stack to a ``calling'' ordered choice operator, and attempts to parse the next
alternative. The next alternative is likely the wrong choice, however. The
recently failed alternative should have matched, but encountered invalid input.
Thus, many other alternatives may fail before an error eventually propagates to
the user. The end result is that the programmer receives an unhelpful error
message that could potentially come from a position many tokens before the
actual problem's origin. This specific challenge has a popular practical
solution in the form of the \emph{farthest failure
heuristic}\cite{ford2002packrat-non-func}. It is based on the observation that
the alternative that should have matched will probably process the longest
prefix of the token stream.

While the farthest failure heuristic addresses the location problem in many
practical situations, it is not a general solution. Worse, the imprecise error
reporting of \acrshort{peg}s has other implications as well. Notably, a common
consideration for error messages is the suggestion of expected tokens to the
user, to provide a rudimentary selection of possible fixes. However, the
compounding failures of \acrshort{peg} choice operators grow the set of expected
tokens, which makes the suggestions in error messages irrelevant.

Intuitively, the problem with \acrshort{peg} error reporting is that
non-terminals deeper down the parse tree have no way of distinguishing between
an error in the input that will ultimately cause the overall parse to fail, and
an error that can be recovered from in an ordered choice operator higher up.

To address this problem, \linkedciteauthor{maidl2013exception} conservatively
extended the \acrshort{peg} formalism with \emph{error
labels}\cite{maidl2013exception}. Error labels semantically\footnote{In the
sense that we are adding semantic information to the process that analyzes
syntax.} stand for errors that a grammar rule can raise when encountering
unexpected input. This differentiates errors caused by nonsensical input from
benign parse failures, and thus solves the problem of accurate error reporting
in \acrlong{peg}s\footnote{At the expense of extensive manual annotations of the
grammar. See \cite{demedeiros2020automatic} for a possible remedy.}. The
extension comprises several components:

\begin{itemize}
	\item The \acrlong{peg} is extended with a finite set of labels $L$.
	\item A special failure label \texttt{fail}, $\texttt{fail} \not \in L$, is
	also added to the grammar. This label indicates a benign failure that can be
	caught by the ordered choice operator.
	\item The grammar of \acrlong{peg} right-hand sides is enriched with the
	\texttt{throw} operator, which takes a label $l \in L$ as an argument. An
	error thrown by \texttt{throw} cannot be caught by ordered choice and thus
	indicates a parse error that should be immediately reported to the user.
	\item Rules are modified to include instances of the \texttt{throw}
	operator.
\end{itemize}

The error label extension is reminiscent of exception handling in ordinary
programming languages, and indeed was originally modelled after it, complete
with an extension of ordered choice playing (the role of)
\texttt{catch}\cite{demedeiros2016parsing}. However, the \texttt{catch}-like
mechanism is not necessary for error recovery, so we will not discuss it
further.

Error labels returned by the parsing process can be mapped to readable error
messages. Because parsing terminates early, the set of expected tokens is kept
accurate. In addition, having a single, obvious point of failure also makes
location tracking trivial.

\subsubsection*{Tracking source locations}

Tracking precise source locations is a common requirement for compilers and is
arguably even more important for language servers. Our lexer provides the
precise span of source code for every token and our preprocessor keeps track of
where a token came from during file inclusion. It is important to track the
entire ``inclusion path'' for every token, which consists of segments of file
identifiers and spans, since a single file can be included multiple times from
multiple locations, with or without detours through other files.

The consideration for the parser here is twofold, namely, to continue to track
the source locations of tokens and grammar productions in the format provided by
the preprocessor, and to maintain incrementality while doing so.

The first consideration requires some care in pattern-matching of tokens -- the
grammar \acrshort{dsl} is not intended to pattern-match on token inclusion
paths. Additionally, an important question is how to assign inclusion paths to
grammar productions. This is trivial for terminals, lookahead, and alterations.
The interesting case is how to handle sequences and repetitions. While a grammar
production could, in theory, store the set of all inclusion paths that it
depends on, doing so is wasteful and not very helpful. Instead, we can inspect
the inclusion paths of the first and the last token in the sequence or
repetition. If these two paths match (meaning they differ at most in the last
span), we can simply use this path, adjusting the final span. This loses
information about tokens from the middle of the sequence, but note that subtrees
will still include it. If the two paths do not match, we can take their longest
common suffix, again adjusting the last span\footnote{Note that our
implementation is incomplete in this regard and presently only reports token
spans local to the parsed file.}.

To maintain incrementality, the parser needs to reuse previous parse results in
response to small changes of the input. Using actual token spans would be
problematic, since a single character change could invalidate the spans of
potentially all tokens in the file. Instead, the parser needs to work with the
relative notion of token count. Every \acrshort{cst} node stores the number of
tokens that it spans (the number of tokens in its subtree). A traversal of the
\acrshort{cst} can then reconstruct the absolute offset of each node without
ever explicitly storing it.

One more thing to note here is that the units of offsets in the \acrshort{cst},
and therefore of the reconstructed spans, are individual tokens. These need to
be converted to spans in the source text before being reported to the user.

\subsection{Abstract syntax trees}

The presented packrat parser produces a \emph{\acrfull{cst}}. A \acrlong{cst} is
a full-fidelity representation of a grammar production, meaning it includes all
tokens and non-terminals, including comments, whitespace, and intermediate
non-terminals introduced to circumvent the constraints of the grammar
\acrshort{dsl}. Preserving full-fidelity syntax trees is important to maintain
the parser's incremental performance and avoid expensive backtracking, but
\acrshort{cst}s include a lot of state that is unimportant to further processing
and analysis steps.

Moreover, the shape and types of these trees are determined by the structure of
the grammar. That is, nodes in a \acrshort{cst} are alterations, repetitions,
sequences, etc. Analysis code would instead prefer a typed \acrshort{api} to
access the semantic, abstract nodes in the syntax tree, without interference
from trivia tokens and non-terminals.

\subsubsection*{Tree abstractions}

These use cases are covered by \acrshort{ast} translation. The \texttt{ast}
module provides three layers of abstraction over \acrshort{cst}s:
\texttt{GreenNode}s, \texttt{SyntaxNode}s, and \texttt{AstNode}s.

A \texttt{GreenNode} wraps a successful parsing result (an
\texttt{ExistingMatch<P4GrammarRules, Token>}) in a reference-counted cell, to
simplify reuse. It provides a \texttt{children} method that returns an iterator
over the children of a node. This iterator is a heap-allocated \texttt{dyn} type
to smooth over differences in the backing \acrshort{cst} node\footnote{Future
extension and optimisation efforts may decide to change the \acrshort{cst}
representation to avoid such indirections in tree traversals.}. The children are
\texttt{GreenNode}s themselves, wrapped transparently by the iterator.
\texttt{GreenNode}s form immutable, functional trees. They are cheap to clone
(thanks to reference counting) and can be structurally shared, although not
between threads\footnote{This could be amended by using atomic reference
counting cells instead.}.

A \texttt{SyntaxNode} is a zipper data structure that maintains a parent pointer
(and therefore the path to the root of the tree) along with the absolute offset
in tokens. It again provides a \texttt{children} method with an iterator that
yields \texttt{SyntaxNode}s. These zippers are useful for traversing
\texttt{GreenNode}s. A \texttt{SyntaxNode} also carries a reference to the
grammar definition and keeps track of the non-terminal that it is in, by storing
it alongside the parent pointer. This is important, as neither the
\texttt{GreenNode} nor the \acrshort{cst} provide this information.

Finally, an \acrshort{ast} node is a typed wrapper around a \texttt{SyntaxNode}.
Unlike the previous two types, it is not a single \texttt{struct}. Instead,
there is a different type for each \acrshort{ast} node, but all of them share a
common interface via the \texttt{AstNode} trait. This trait provides methods to
cast a \texttt{SyntaxNode} to a particular \texttt{AstNode} type, to access the
backing \texttt{SyntaxNode}, and to intuit the node's offset and span.

Many \acrshort{ast} nodes share the binary representation with their underlying
\texttt{SyntaxNode}s, meaning they only have a single field of the
\texttt{SyntaxNode} type. Additionally, the implementations of the
\texttt{AstNode} trait are often structurally similar. To avoid code duplication
and simplify maintenance, the \texttt{ast} module provides the
\texttt{ast\_node!} macro, through which most \acrshort{ast} nodes are defined.

\begin{lstlisting}[
	language=Rust,
	tabsize=4,
	float,
	caption={The signature of the \texttt{ast\_node!} macro.},
	label={lst:ast_node_macro},
]
macro_rules! ast_node {
	($non_terminal:ident $(, methods: $($method:ident),+)?) => {
		paste! {
			// see later listings for the body of this macro
		}
	};
}
\end{lstlisting}

The signature and top level of the macro are shown in
Listing~\ref{lst:ast_node_macro}. Note the inclusion of the \texttt{paste!}
block: the \texttt{paste} crate\footnote{\url{https://crates.io/crates/paste}}
provides means for converting the case of identifiers. We use this throughout
the \texttt{ast\_node!} macro to generate the \texttt{struct} name from the name
of the non-terminal. Rust types are conventionally written in camel case while
methods, as well as our non-terminals, have snake case names.

The body of the macro defines the \texttt{struct} and implements the
\texttt{AstNode} trait for it. To generate documentation comments, the macro's
body gives \acrshort{ast} node \texttt{struct}s the \texttt{doc} attribute. The
line can be seen below, it was omitted from
Listing~\ref{lst:ast_node_macro_body} to avoid confusing the lexer:

\texttt{\#[doc = "AST node for [`P4GrammarRules::" \$non\_terminal "`]."]}

\begin{lstlisting}[
	language=Rust,
	tabsize=4,
	float,
	caption={The main body of the \texttt{ast\_node!} macro generates newtypes for \texttt{SyntaxNode}s and implements \texttt{AstNode} for them.},
	label={lst:ast_node_macro_body},
]
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct [<$non_terminal:camel>] {
	syntax: SyntaxNode,
}

impl AstNode for [<$non_terminal:camel>] {
	fn can_cast(node: &SyntaxNode) -> bool {
		node.kind() == P4GrammarRules::$non_terminal
	}

	fn cast(node: SyntaxNode) -> Option<Self> {
		if node.kind() == P4GrammarRules::$non_terminal {
			Some(Self { syntax: node })
		} else {
			None
		}
	}

	fn syntax(&self) -> &SyntaxNode { &self.syntax }
}

// continues with optional methods
\end{lstlisting}

Finally, the invocation of \texttt{ast\_node!} can optionally contain a list of
methods to implement on the \texttt{struct} itself, outside the scope of the
\texttt{AstNode} trait. The implementation of this functionality can be seen in
Listing~\ref{lst:ast_node_macro_body_methods}. These generated methods provide
type-safe access to children while transparently handling both the skipping of
trivia nodes and invalid underlying \acrshort{cst}s. Each returns an iterator
without a guarantee of exactly how many children will be iterated over,
smoothing over issues of arity. It is up to the users of this \acrshort{api} to
detect and report such errors.

\begin{lstlisting}[
	language=Rust,
	tabsize=4,
	float,
	caption={The optional methods section of the \texttt{ast\_node!}
	macro's body.},
	label={lst:ast_node_macro_body_methods},
]
$(impl [<$non_terminal:camel>] {
	$(
		#[doc = "Fetch the `" $method "` child of this node."]
		pub fn $method(
			&self
		) -> impl Iterator<Item = [<$method:camel>]> {
			fn go(
				node: SyntaxNode
			) -> Box<dyn Iterator<Item = SyntaxNode>> {
				match node.trivia_class() {
					TriviaClass::SkipNodeAndChildren =>
						Box::new(std::iter::empty()),
					TriviaClass::SkipNodeOnly =>
						Box::new(node.children().flat_map(go)),
					TriviaClass::Keep =>
						Box::new(std::iter::once(node)),
				}
			}

			self.syntax()
				.children()
				.flat_map(go)
				.filter_map([<$method:camel>]::cast)
		}
	)+
})?
\end{lstlisting}

The entire \acrshort{ast} translation layer was inspired by the internals of
rust-analyzer\footnote{\url{https://rust-analyzer.github.io/}}, an
\acrshort{lsp}-compliant language server for Rust. While rust-analyzer's syntax
tree manipulations work with a different underlying representation and rely on a
recursive descent parser, the high-level interfaces, the split between
\texttt{GreenNode}s, \texttt{SyntaxNode}s, and \texttt{AstNode}s, as well as the
heavy reliance on code generation, are all motivated directly by rust-analyzer's
developer documentation. We would like to thank the rust-analyzer developers for
maintaining a comprehensive high-level documentation of their approach and
including known alternative approaches, e.g. from Roslyn and IntelliJ, that
other developers can learn from.

\subsubsection*{Identifying trivia nodes}

The reader will observe that we have carelessly included a previously
undiscussed type in the \texttt{ast\_node!} macro's body: the
\texttt{TriviaClass} \texttt{enum}.
Listing~\ref{lst:ast_node_macro_body_methods} makes it quite clear that the
\texttt{TriviaClass} type determines which subtrees to iterate over when
invoking a generated method of an \texttt{AstNode} type.

Our grammar \acrshort{dsl} has a few extra features. The author of a grammar can
identify the trivia non-terminals by including \emph{annotations} in the grammar
definition. Listing~\ref{lst:trivia_annotation} shows how to use these
annotations. By default, every non-terminal has a \texttt{TriviaClass} of
\texttt{Keep}, which means it will be enumerated by an \texttt{AstNode}'s
iterator.

It is worth noting that the macro for defining the \acrshort{p4} grammar is also
the source of truth for the set of non-terminals: it simultaneously defines the
\texttt{P4GrammarRules} \texttt{enum}. This is very useful for developers, since
it avoids code duplication and enables features like go-to definition or find
references right in the grammar definition. Using an \texttt{enum} to represent
non-terminals is also a requirement for exhaustiveness checking, in case any
later steps in the pipeline require it. The grammar \acrshort{dsl} forwards
documentation comments above productions to the variants of
\texttt{P4GrammarRules}.

\begin{lstlisting}[
	language=Rust,
	tabsize=4,
	float,
	caption={An example of trivia annotations and doc comments in the grammar
	\acrshort{dsl}.},
	label={lst:trivia_annotation},
]
grammar! {
	@SkipNodeAndChildren at_symbol close_paren;
	@SkipNodeOnly maybe_direction parameter_comma;

	/// Semantic non-terminal that marks an identifier as a definition.
	///
	/// For example, in `parser MyParser<T>(inout T x) { }`,
	/// `MyParser`, `T`, and `x` are all definitions,
	/// and possible targets for go-to definition.
	definition => ident;

	// other non-terminals omitted
}
\end{lstlisting}
