%---------------%
\chapter{Design}
%---------------%

\todo[inline]{the design of the p4 language server, what it learned from
rust-analyzer, etc}

\begin{itemize}
	\item intention to do everything in-editor, Rust to WebAssembly
	\item cannot reuse the open-source frontend, not error-tolerant, difficult
	to work with, poor diagnostics
	\item needs to adapt to various backends and architectures easily
	\item open-source project, no point in keeping it closed, want to encourage
	contributions from the community
\end{itemize}

The high-level architecture of the \acrshort{p4} Analyzer project marks a
departure from conventional language server designs in that it primarily targets
WebAssembly and aims to run entirely within the Visual Studio Code editor. This
decision makes installation simpler for the end-user, cross-platform support
easier for the developers, and security policy conformance trivial for any
security teams involved.

The main mode of operation is thus as follows: the language server runs in a
WebAssembly worker of the \acrshort{p4} Analyzer VS Code extension. The
extension itself defines a simple TextMate\todo{reference} grammar specification
and serves as a thin client for the server. The main bulk of \acrshort{lsp}
functionality is delegated to the editor. VS Code forwards edits to open files
to the language server, which updates its model of the workspace. When VS Code
asks for completions, hover, diagnostics, or other features, the analyzer
recomputes necessary information on-demand and responds appropriately.

In addition to the WebAssembly executable, the \acrshort{p4} Analyzer project
also compiles to a native binary that executes in a standalone process and
communicates with an arbitrary \acrshort{lsp}-compliant client over a socket.
However, the standalone language server requires support for certain features
related to filesystem functionality that fall outside the protocol
specification. We will discuss these in-depth later\todo{make sure this is
indeed the case, put a reference here}.

%-----------------------------------------------%
\section{The \pdfacrshort{p4} Analyzer pipeline}
%-----------------------------------------------%

The first step in our pipeline is lexical analysis. Somewhat unconventionally,
our lexer produces tokens even for the preprocessor (i.e. it analyses
preprocessor directives). Our preprocessor then operates at the lexeme level,
rather than running separately as the first step. This requires a
reimplementation of the preprocessor, which is already necessitated by
fault-tolerance and WebAssembly support requirements anyway. On the upside, a
custom preprocessor simplifies tracking of source positions, which are crucial
for accurate diagnostics.

\subsection{Lexical analysis}

The \pfs specification defines a \acrshort{yacc}\todo{use a glossary entry
instead of an acronym}/Bison grammar for the language. However, this grammar has
several flaws.

For example, it reuses the \texttt{parserTypeDeclaration} non\-terminal in
\texttt{parserDeclaration}s but imposes extra restrictions: a parser declaration
may not be generic. This requires checking the child production outside the
grammar specification.

However, the primary issue is that the grammar design does not maintain a clean
separation between a parser and a lexer and requires these two components to
collaborate.

\begin{displayquote}
	\textit{The grammar is actually ambiguous, so the lexer and the parser must
	collaborate for parsing the language. In particular, the lexer must be able
	to distinguish two kinds of identifiers:}

	\begin{itemize}
		\item \textit{Type names previously introduced (\texttt{TYPE\_IDENTIFIER}
		tokens)}
		\item \textit{Regular identifiers (\texttt{IDENTIFIER} token)}
	\end{itemize}

	\textit{The parser has to use a symbol table to indicate to the lexer how to
	parse subsequent appearances of identifiers.}

	-- \citetitle{p416:v123:spec} \cite{p416:v123:spec}
\end{displayquote}

The specification goes on to show an example where the lexer output depends on
the parser state and mentions that the presented grammar ``\textit{has been heavily
influenced by limitations of the Bison parser generator tool.}''

The tight coupling between the lexer and the parser, as well as the decision to
remain in the confines of an outdated parser generator despite its many
drawbacks, are in our opinion examples of poor design for a language born in the
twenty-first century. We have elected not to follow this ambiguous grammar
specification in the \acrshort{p4} Analyzer project and instead build a pipeline
that is tolerant to invalid input to the fullest extent possible, while
accepting the same language.

Our lexer's task is to convert the input string into a stream of lexemes. The
lexer is a standalone finite state machine independent of any later stages in
the pipeline. It has a secondary output for reporting diagnostics, but this side
channel is write-only.

\subsubsection*{Error tolerance}

Error tolerance at the lexer level means proceeding with lexeme stream
generation despite nonsensical input. We emit a special error token whenever
such input is encountered. Additionally, the lexer validates numeric constants,
which can specify width, base, and signedness\todo{is this a word?}. These
properties could be out of bounds for a given literal. In these cases, the lexer
produces\todo{"should produce" language instead? since this is a design chapter,
wink wink?} a valid token anyway but logs an error-level diagnostic, which is
reported to the user once lexing completes.


\subsection{The preprocessor}

\pfs requires support for a preprocessor, very similar to the C preprocessor,
directly in the specification. However, it does not ask implementors to support
the entirety of \texttt{cpp}. Notably, only simple, parameter-less macros are
allowed. This is already enough to necessitate running the preprocessor before
starting the parser, however. Consider the code in
Listing~\ref{lst:p4pp}\todo{p4 syntax would be nice}. These examples show how
grammatically invalid code may become valid and vice versa, based only on the
right-hand sides of preprocessor macros.

An important consideration for a correct implementation of preprocessor
directives is their context-sensitive nature. Expressions for conditional
inclusion in directives \texttt{\#if}, \texttt{\#elif}, and \texttt{\#ifdef} are
themselves subject to macro substitution and thus have to be kept in plain text
or lexeme form until their evaluation.

One more thing to note here is the mechanism of document inclusion. Before
analysing a \pfs source file (at least to some degree), the full extent of its
dependencies is unknown and arbitrary. The language has no module system and
imposes no restriction on the paths a source file can include. This poses a
challenge for lexeme-level preprocessors, as a file needs to be lexed before it
can be included. To deal with this, a correct implementation should collect the
paths a source file can depend on, lex their contents, and include their lexemes
in the preprocessed lexeme stream. This is of particular note in our
implementation, as the collection of dependencies reports this dependency set to
the editor to set up filesystem-level watches. Subsequent edits to the
dependencies, or even to the dependency set itself, can be processed
incrementally.

\subsubsection*{Error tolerance}

Error tolerance in the preprocessor means reporting errors and warnings about
malformed input to the user while continuing to interpret directives in the
input stream on a best-effort basis. Mistakes in preprocessor directives come in
several flavours.

The directive itself may be malformed, either due to a typo in its name or a
problem in some of its arguments. The former case will simply be lexed as an
unrecognized directive and reported as such. It is possible to suggest fixes for
common typos to the user. A problem in the directive's argument or arguments
needs to be resolved based on its meaning. For example, an \texttt{\#include}
directive could point to a non-existent file, the preprocessor should then
report this error and proceed as if the file were not included. This is likely
to lead to further errors down the road, but without knowledge of the referenced
file's contents, it is the best a preprocessor can do.

Another class of errors is semantic and context-sensitive in nature: a directive
may be used in the incorrect context or missing where it is expected. For
example, a user may forget to add an \texttt{\#endif} directive, or include more
than one \texttt{\#else} directive for a condition. Unfortunately, guessing the
user's intention when faced with any syntactic or semantic problems in the input
is a tall order. No guarantees of optimality can be given, as is often the case
with similar heuristics. In the duplicate \texttt{\#else} problem, the
preprocessor could be reasonably expected to either skip over the first
\texttt{\#else}'s body, the second \texttt{\#else}'s body, or assume either of
the directives was inserted by accident and pretend it is not a part of the
input stream. We choose to skip the second \texttt{\#else}'s body in our design,
but other strategies are equally valid\todo{maybe it'd be better to look at some
data from users. But does this matter enough to bother with a study?}.

\begin{lstlisting}[
	caption={~\pfs preprocessor example},
	label=lst:p4pp,
	captionpos=t,
	tabsize=4,
	float,
	abovecaptionskip=-\medskipamount,
	belowcaptionskip=\medskipamount,
	language=c
]
#define op +
// #define op 2

#define paren )

header h {
	bit<1> field;
}

control pipe(inout h hdr) {
	Checksum16() ck;
	apply {
		// arithmetic expression could be invalid
		h.field = 1 op 3;
		// a parse without prior macro substitution would fail
		ck.clear( paren;
		// this would parse correctly, but macro substitution
		// will reveal a parse error
		ck.update(op);
	}
}
\end{lstlisting}


\subsection{The parser}

The next natural step in the pipeline is the act of finding the productions of a
\pfs grammar that match the preprocessed input program; parsing. While the steps
up to this point are fairly simplistic and efficient, parsing is a
resource-intensive process. A language server is expected to provide real-time
feedback to the developer, including auto-completion suggestions updated with
every keypress\todo{the lexer can identify a caret inside a (possibly
unfinished!) comment or string. Maybe we can learn from that for naive
lexer-based auto-completion?}. Low latency is crucial to the end-user and the
parser lies on every critical path from user input to high-level results shown
in the editor's interface. At the same time, a typical \pfs program is likely to
consist of a long prefix that does not change between edits and a
user-maintained suffix that changes frequently. This is because a \pfs program
usually begins with \texttt{\#include} directives referencing platform-specific
files with constants, error codes, \extern{} definitions and other shared code.
These constraints and conditions are a very good fit for the field of
\emph{incremental parsing}.

An incremental parser aims to reuse previously computed information about the
input in response to small perturbations. Our parser specifically builds on
incremental packrat parsing\cite{dubroy2017incremental-packrat-parsing}, which
places few constraints on grammar design and is easy to implement in an
extensible manner.

Packrat parsing\cite{ford2002packrat} is a linear time algorithm for recognizing
productions of a \acrfull{peg}. It relies heavily on memoization to avoid costly
backtracking, at the expense of memory overhead.
\linkedciteauthor{dubroy2017incremental-packrat-parsing} augment the packrat
memoization table to support incrementality. The result is a parser that is at
once simple, general, incremental, and efficient.

Our packrat parser conceptually handles \acrlong{peg}s\cite{ford2004parsing}, a
class of unambiguous grammars for context-free languages. \acrshort{peg}s are
syntactically similar to \acrlong{cfg}s. However, the choice operator in
\acrshort{cfg}s is ambiguous, whereas \acrshort{peg}s use ordered choice, which
greedily attempts to match alternatives in order. The right-hand side of a
\acrlong{peg} rule can also contain predicates, which attempt to match without
consuming input. Predicates are useful for positive and negative look\-ahead.

Our design differentiates between a generic parser library and a parser built on
it. Grammars are defined using a small \acrshort{dsl} implemented with Rust's
declarative macros. An example can be seen in
Listing~\ref{lst:grammar-dsl-example}. The~\texttt{grammar!} macro expands to a
data structure representing the grammar itself. This structure can be passed to
a smart constructor, which validates the grammar\footnote{Ensuring all
referenced non-terminals are in fact defined, and that \texttt{start} is
present.} and returns a parser. The implementation interprets the grammar
\acrshort{dsl} at runtime.

If future testing and development necessitate optimization of the parser, there
is room to build a parser compiler for the \acrshort{dsl} and generate a more
efficient solution from the same grammar. Procedural Rust macros\footnote{While
declarative macros can only perform a very restricted set of rewriting
operations on token trees, procedural macros can run arbitrary Rust code during
expansion.} could take care of integrating the parser compiler into the build
process.

\subsubsection*{Incremental updates}

Since the parser is required to process incremental updates to the input
sequence, it is not simply a function from a sequence of tokens to a parse tree.
Rather, the parser takes a reference to a read-write lock of the input. It
defines an \texttt{apply\_edit} method that acquires a write lock of the input
sequence, applies the change, invalidates relevant entries in the memoization
table, and releases the lock.

To initiate parsing, the user invokes the \texttt{\_match} method\footnote{Named
as such because \texttt{match} is a Rust keyword.}, which acquires a read lock
on the input for the duration of parsing.


\begin{lstlisting}[
	caption={~Example grammar in our \acrshort{dsl}.},
	label=lst:grammar-dsl-example,
	captionpos=t,
	tabsize=4,
	float,
	abovecaptionskip=-\medskipamount,
	belowcaptionskip=\medskipamount,
	language=c
]
grammar! {
	// The initial non-terminal is called `start`
	start => p4program;
	// The postfix `rep` operator corresponds to Kleene star
	ws => whitespace rep;
	// Non-terminals can expand to terminals by wrapping the terminal in
	// parentheses
	whitespace => (Token::Whitespace);

	// The right hand side can also be a sequence separated by commas
	p4program => ws, top_level_decls, ws;
	// ...or a choice separated by pipes
	top_level_decls =>
		top_level_decls_rep | top_level_decls_end | nothing;
	top_level_decls_rep => top_level_decl, ws, top_level_decls;
	top_level_decls_end => (Token::Semicolon);

	direction => dir_in | dir_out | dir_inout;
	// Rules can also match tokens against an arbitrary Rust pattern,
	// which is useful for identifying soft keywords
	dir_in    => { Token::Identifier(i) if i == "in" };
	dir_out   => { Token::Identifier(i) if i == "out" };
	dir_inout => { Token::Identifier(i) if i == "inout" };
}
\end{lstlisting}


\subsubsection*{Error reporting}

Error handling in parsing is far more nuanced than in any of the previous steps,
which is not surprising, considering the relative complexity of the languages
that the individual steps recognize and process. A good parser should attempt to
provide as much feedback as possible to the user, even when faced with
unexpected tokens. It is not enough to simply stop at the first error, and it is
not enough to be imprecise about the locations of problems in the source file.
Both of these considerations pose some challenges.

The desire to continue parsing malformed input to provide feedback to the
developer has a long history\cite{graham1975practical}. Error recovery has been
studied at length in \acrlong{peg}s as well\cite{redziejowski2009mouse,
maidl2013exception, demedeiros2016parsing, demedeiros2018syntax,
demedeiros2020automatic}. The \acrshort{peg} case is interesting, because a
packrat parser relies on failures to guide choice selection. By the time a
parsing failure propagates to the starting non-terminal, the information about
the context that led to it is lost.

Specifically, when encountering unexpected input, a packrat parser unwinds the
stack to a ``calling'' ordered choice operator, and attempts to parse the next
alternative. The next alternative is likely the wrong choice, however. The
recently failed alternative should have matched, but encountered invalid input.
Thus, many other alternatives may fail before an error eventually propagates to
the user. The end result is that the programmer receives an unhelpful error
message that could potentially come from a position many tokens before the
actual problem's origin. This specific challenge has a popular practical
solution in the form of the \emph{farthest failure
heuristic}\cite{ford2002packrat-non-func}. It is based on the observation that
the alternative that should have matched will probably process the longest
prefix of the token stream.

While the farthest failure heuristic addresses the location problem in many
practical situations, it is not a general solution. Worse, the imprecise error
reporting of \acrshort{peg}s has other implications as well. Notably, a common
consideration for error messages is the suggestion of expected tokens to the
user, to provide a rudimentary selection of possible fixes. However, the
compounding failures of \acrshort{peg} choice operators grow the set of expected
tokens, which makes the suggestions in error messages irrelevant.

Intuitively, the problem with \acrshort{peg} error reporting is that
non-terminals deeper down the parse tree have no way of distinguishing between
an error in the input that will ultimately cause the overall parse to fail, and
an error that can be recovered from in an ordered choice operator higher up.

To address this problem, \citeauthor{maidl2013exception} conservatively extended
the \acrshort{peg} formalism with \emph{error labels}\cite{maidl2013exception}.
Error labels semantically\footnote{In the sense that we are adding semantic
information to the process that analyzes syntax.} stand for errors that a
grammar rule can raise when encountering unexpected input. This differentiates
errors caused by nonsensical input from benign parse failures, and thus solves
the problem of accurate error reporting in \acrlong{peg}s\footnote{At the
expense of extensive manual annotations of the grammar. See
\cite{demedeiros2020automatic} for a possible remedy.}. The extension comprises
several components:

\todo[inline]{we actually do want a full syntax for \acrlong{peg}s}

\begin{itemize}
	\item The \acrlong{peg} is extended with a finite set of labels $L$.
	\item A special failure label \texttt{fail}, $\texttt{fail} \not \in L$, is
	also added to the grammar. This label indicates a benign failure that can be
	caught by the ordered choice operator.
	\item The grammar of \acrlong{peg} right-hand sides is enriched with the
	\texttt{throw} operator, which takes a label $l \in L$ as an argument. An
	error thrown by \texttt{throw} cannot be caught by ordered choice and thus
	indicates a parse error that should be immediately reported to the user.
	\item Rules are modified to include instances of the \texttt{throw}
	operator.
\end{itemize}

The error label extension is reminiscent of exception handling in ordinary
programming languages, and indeed was originally modelled after it, complete
with an extension of ordered choice playing (the role of)
\texttt{catch}\cite{demedeiros2016parsing}. However, the \texttt{catch}-like
mechanism is not necessary for error recovery, so we will not discuss it
further.

Error labels returned by the parsing process can be mapped to readable error
messages. Because the parsing process terminates early, the set of expected
tokens is kept accurate. In addition, having a single, obvious point of failure
also makes location tracking trivial.

\subsubsection*{Tracking source locations}

However, our setting complicates location tracking. Source locations are an
important consideration for any incremental parser. A naive implementation, that
includes the source locations in the parse tree, leads to a linear amount of
work per edit, as the parser must recompute the source location of every node.
This would largely defeat the purpose of incremental parsing. At the same time,
source locations are important for features such as go to definition, or
diagnostics generated by semantic analysis in subsequent processing steps. While
we could keep all positions relative,

Fortunately, the nature of the incremental parsing approach of
\citeauthor{dubroy2017incremental-packrat-parsing} permits an interesting hybrid
between

\todo[inline]{mention that we can trade off memory for speed in the preprocessor
by controlling memoization in a fine-grained manner using Salsa.}

\todo[inline]{ example of complex preprocessor logic
	% https://github.com/Princeton-Cabernet/p4-projects/blob/10a4b9ce25b725b5269ba857470f63c96f82c645/AES.p4app/AES.p4#L18
}
